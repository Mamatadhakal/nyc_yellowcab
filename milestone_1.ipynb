{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47952680-dde6-407f-b395-512a6ce9a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd   # Import pandas library, mainly used for handling tabular data (DataFrames, Series, etc.)\n",
    "import numpy as np    # Import NumPy library, provides powerful numerical operations and array handling\n",
    "\n",
    "# File and paths\n",
    "from pathlib import Path    # Import Path from pathlib to work with filesystem paths in an object-oriented way\n",
    "import shutil               # Import shutil to perform high-level file operations like copy, move, delete\n",
    "import urllib.request       # Import urllib.request to handle opening URLs and downloading data from the web\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt  # Import matplotlib's pyplot module, commonly used for creating plots and visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada95a2a-64eb-478e-bad4-7a49e539ce37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/mamat/Documents/nyc')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path   # Import Path class for working with filesystem paths\n",
    "\n",
    "# Loop through a list of folder names and create them if they don't already exist\n",
    "for p in [\"data/raw\", \"data/staging\", \"data/warehouse\"]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)  \n",
    "    # Path(p) creates a Path object for each directory string\n",
    "    # .mkdir() makes the directory\n",
    "    # parents=True allows creation of parent directories if they don’t exist\n",
    "    # exist_ok=True avoids errors if the directory already exists\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "Path(\".\").absolute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b21883-66e1-40bc-a44a-e56f5e4cfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil   # Import shutil to perform high-level file operations like copy and move\n",
    "\n",
    "# Define the source file path (where the file is originally located)\n",
    "src = Path(r\"C:\\Users\\mamat\\Downloads\\yellow_tripdata_2024-01.parquet\")  \n",
    "\n",
    "# Define the destination file path (where we want to copy the file to)\n",
    "dst = Path(\"data/raw/yellow_tripdata_2024-01.parquet\")\n",
    "\n",
    "# Check if the source file exists and the destination file does not yet exist\n",
    "if src.exists() and not dst.exists():\n",
    "    shutil.copy2(src, dst)  \n",
    "    # copy2() copies the file along with its metadata (timestamps, permissions, etc.)\n",
    "\n",
    "# Print confirmation: whether the file exists at the destination, and show the path\n",
    "print(\"Raw file at:\", dst.exists(), dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af80055-eb9d-4f3c-b1dd-0296fdcb215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # Import pandas for data manipulation and analysis\n",
    "\n",
    "# Define the path to the raw Parquet file\n",
    "raw_path = \"data/raw/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df = pd.read_parquet(raw_path)\n",
    "\n",
    "# Print the number of rows in the DataFrame\n",
    "print(\"rows:\", len(df))\n",
    "\n",
    "# Display the first 5 rows of the DataFrame for a quick preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498e8c-df27-46cf-be9d-90581242dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()   # Work on a copy to avoid modifying the original DataFrame\n",
    "\n",
    "    # --- timestamps to datetime ---\n",
    "    for c in [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")  \n",
    "        # Convert pickup/dropoff columns to datetime, invalid values become NaT\n",
    "\n",
    "    # --- derived fields ---\n",
    "    df[\"duration_min\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60.0\n",
    "    # Trip duration in minutes\n",
    "    \n",
    "    df[\"pickup_hour\"]   = df[\"tpep_pickup_datetime\"].dt.hour   # Extract hour of pickup\n",
    "    df[\"pickup_date\"]   = df[\"tpep_pickup_datetime\"].dt.date   # Extract date (no time)\n",
    "    df[\"year\"]          = df[\"tpep_pickup_datetime\"].dt.year   # Extract year\n",
    "    df[\"month\"]         = df[\"tpep_pickup_datetime\"].dt.month  # Extract month\n",
    "\n",
    "    # --- required columns present ---\n",
    "    required = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DOLocationID\",\"fare_amount\",\"total_amount\"]\n",
    "    df = df.dropna(subset=required)  \n",
    "    # Remove rows missing critical fields\n",
    "\n",
    "    # --- logical & outlier filters ---\n",
    "    df = df[(df[\"duration_min\"] >= 1) & (df[\"duration_min\"] <= 180)]         \n",
    "    # Keep trips between 1 min and 3 hours\n",
    "    \n",
    "    df[\"trip_distance\"] = df[\"trip_distance\"].fillna(0)  \n",
    "    # Replace missing distances with 0\n",
    "    \n",
    "    df = df[(df[\"trip_distance\"] >= 0) & (df[\"trip_distance\"] <= 100)]       \n",
    "    # Keep trips within 0–100 miles\n",
    "    \n",
    "    df = df[(df[\"fare_amount\"] >= 0) & (df[\"total_amount\"] >= 0)]            \n",
    "    # Fares and totals must be non-negative\n",
    "\n",
    "    # --- fare per mile guardrails (ignore when distance==0) ---\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        fpm = df[\"fare_amount\"] / df[\"trip_distance\"].replace({0: np.nan})  \n",
    "        # Compute fare per mile; avoid division by zero\n",
    "    \n",
    "    df[\"fare_per_mile\"] = fpm\n",
    "    df = df[(df[\"fare_per_mile\"].isna()) | ((df[\"fare_per_mile\"] >= 0.5) & (df[\"fare_per_mile\"] <= 25))]\n",
    "    # Keep only reasonable fares per mile (0.5–25 USD), allow NaN when distance=0\n",
    "\n",
    "    # --- de-duplicate ---\n",
    "    before = len(df)\n",
    "    dedup_cols = [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DOLocationID\",\"total_amount\"]\n",
    "    df = df.drop_duplicates(subset=dedup_cols, keep=\"first\")  \n",
    "    # Remove duplicates based on key trip identifiers\n",
    "    print(f\"Deduplicated: removed {before - len(df)} rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function\n",
    "cleaned = clean_add_features(df)\n",
    "\n",
    "# Compare sizes before and after cleaning\n",
    "len(df), len(cleaned), cleaned.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd6dde-c5f5-469b-b6da-ca83fdf1bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory where cleaned parquet files will be saved\n",
    "out_dir = \"data/warehouse/yellow_clean\"\n",
    "\n",
    "# Save the cleaned DataFrame as partitioned Parquet files\n",
    "# - engine=\"pyarrow\": use the PyArrow backend (required for partitioning in pandas >= 2.0)\n",
    "# - index=False: don’t write the DataFrame index as a column\n",
    "# - partition_cols=[\"year\",\"month\"]: split the dataset into folders by year/month (data lake style)\n",
    "cleaned.to_parquet(out_dir, engine=\"pyarrow\", index=False, partition_cols=[\"year\",\"month\"])\n",
    "\n",
    "# Just return the output directory string so you can see where the files are stored\n",
    "out_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7ecc6-bc5e-4394-89d0-b9b9b08bc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory where the partitioned parquet files were written\n",
    "clean_dir = \"data/warehouse/yellow_clean\"\n",
    "\n",
    "# Read back the dataset\n",
    "# - pandas will automatically detect the partitioned structure (year=..., month=...)\n",
    "# - engine=\"pyarrow\" is required since that’s what we used to write it\n",
    "df_clean = pd.read_parquet(clean_dir, engine=\"pyarrow\")\n",
    "\n",
    "# Show how many rows were loaded and preview the first 2 rows\n",
    "len(df_clean), df_clean.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e3aa7-51f9-482c-9146-62886d53b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_by_hour = (\n",
    "    df_clean\n",
    "    # 1. Create a new column \"hour\" from pickup timestamp\n",
    "    .assign(hour=df_clean[\"tpep_pickup_datetime\"].dt.hour)\n",
    "\n",
    "    # 2. Group rows by pickup hour\n",
    "    .groupby(\"hour\", as_index=False)\n",
    "\n",
    "    # 3. Count number of trips per hour (size = number of rows in each group)\n",
    "    .size()\n",
    "\n",
    "    # 4. Rename the count column from \"size\" → \"trips\"\n",
    "    .rename(columns={\"size\": \"trips\"})\n",
    "\n",
    "    # 5. Sort results by hour in ascending order (0..23)\n",
    "    .sort_values(\"hour\")\n",
    ")\n",
    "\n",
    "# Show the hourly trip counts DataFrame\n",
    "trips_by_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41678eaf-4e77-4892-8bbe-5c6659bc7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_rev = (\n",
    "    df_clean\n",
    "    # 1. Create a new column \"ym\" (year-month) from pickup timestamp\n",
    "    #    - .dt.to_period(\"M\") converts datetime → monthly period\n",
    "    #    - .astype(str) makes it a string like \"2024-01\"\n",
    "    .assign(ym=df_clean[\"tpep_pickup_datetime\"].dt.to_period(\"M\").astype(str))\n",
    "\n",
    "    # 2. Group by the year-month column\n",
    "    .groupby(\"ym\", as_index=False)\n",
    "\n",
    "    # 3. Aggregate trip metrics:\n",
    "    #    - total revenue = sum of total_amount\n",
    "    #    - average fare per trip = mean of total_amount\n",
    "    #    - trips = number of rows in the group\n",
    "    .agg(total_revenue=(\"total_amount\", \"sum\"),\n",
    "         avg_total=(\"total_amount\", \"mean\"),\n",
    "         trips=(\"total_amount\", \"size\"))\n",
    "\n",
    "    # 4. Sort by year-month for chronological order\n",
    "    .sort_values(\"ym\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "monthly_rev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01430f0-223a-454d-b4c4-d3f5cbe83c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define a Path object pointing to the February 2024 Yellow Taxi parquet file\n",
    "raw_feb = Path(r\"C:\\Users\\mamat\\Downloads\\yellow_tripdata_2024-02.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b805e-e0c0-4062-8f7d-8bac21ee2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination path inside your project data/raw folder\n",
    "dst = Path(\"data/raw/yellow_tripdata_2024-02.parquet\")\n",
    "\n",
    "# If the source parquet file exists (in Downloads) and is not already copied...\n",
    "if raw_feb.exists() and not dst.exists():\n",
    "    import shutil\n",
    "    shutil.copy2(raw_feb, dst)   # Copy file along with metadata (timestamps, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b87c08-0ae6-47be-b539-1e128e681c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the February parquet file into a pandas DataFrame\n",
    "df_feb = pd.read_parquet(dst)\n",
    "\n",
    "# Print how many rows (trips) are in the February raw dataset\n",
    "print(\"Rows in Feb raw file:\", len(df_feb))\n",
    "\n",
    "# Show the first 5 rows so you can inspect the columns and data\n",
    "df_feb.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4166550-38d1-4223-8bcd-4a47f4507f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Make a copy so we don't modify the original DataFrame passed in\n",
    "    df = df.copy()   \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 1. Convert timestamp columns to proper datetime objects\n",
    "    # -------------------------------------------------------------------\n",
    "    # - Ensures calculations like time differences will work\n",
    "    # - \"errors='coerce'\" turns invalid values into NaT (Not-a-Time)\n",
    "    for c in [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 2. Add derived / engineered features from pickup time\n",
    "    # -------------------------------------------------------------------\n",
    "    # Trip duration in minutes (dropoff minus pickup, converted from seconds to minutes)\n",
    "    df[\"duration_min\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60.0\n",
    "    \n",
    "    # Hour of day when the trip started (0–23)\n",
    "    df[\"pickup_hour\"]   = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    \n",
    "    # Calendar date (YYYY-MM-DD) of the pickup\n",
    "    df[\"pickup_date\"]   = df[\"tpep_pickup_datetime\"].dt.date\n",
    "    \n",
    "    # Year of the pickup (useful for partitioning and trend analysis)\n",
    "    df[\"year\"]          = df[\"tpep_pickup_datetime\"].dt.year\n",
    "    \n",
    "    # Month of the pickup (1–12)\n",
    "    df[\"month\"]         = df[\"tpep_pickup_datetime\"].dt.month\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 3. Drop rows that are missing required fields\n",
    "    # -------------------------------------------------------------------\n",
    "    # These columns are essential to define a valid trip.\n",
    "    required = [\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\",\n",
    "        \"PULocationID\",    # pickup location\n",
    "        \"DOLocationID\",    # dropoff location\n",
    "        \"fare_amount\",     # fare paid\n",
    "        \"total_amount\"     # total charged (fare + extras)\n",
    "    ]\n",
    "    df = df.dropna(subset=required)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 4. Apply logical filters and remove outliers\n",
    "    # -------------------------------------------------------------------\n",
    "    # Keep only trips lasting between 1 minute and 3 hours (180 minutes)\n",
    "    df = df[(df[\"duration_min\"] >= 1) & (df[\"duration_min\"] <= 180)]\n",
    "    \n",
    "    # Replace missing distances with 0 (assume unreported distance means 0 miles)\n",
    "    df[\"trip_distance\"] = df[\"trip_distance\"].fillna(0)\n",
    "    \n",
    "    # Keep trips where distance is between 0 and 100 miles\n",
    "    df = df[(df[\"trip_distance\"] >= 0) & (df[\"trip_distance\"] <= 100)]\n",
    "    \n",
    "    # Keep trips where fare and total amount are non-negative\n",
    "    df = df[(df[\"fare_amount\"] >= 0) & (df[\"total_amount\"] >= 0)]\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 5. Sanity check: Fare per mile\n",
    "    # -------------------------------------------------------------------\n",
    "    # - If distance is 0, skip calculation (replace with NaN).\n",
    "    # - Keep only trips where fare per mile is between $0.50 and $25.\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        fpm = df[\"fare_amount\"] / df[\"trip_distance\"].replace({0: np.nan})\n",
    "    \n",
    "    df[\"fare_per_mile\"] = fpm\n",
    "    \n",
    "    # Keep trips with reasonable fare per mile OR NaN (for zero distance trips)\n",
    "    df = df[\n",
    "        (df[\"fare_per_mile\"].isna()) |\n",
    "        ((df[\"fare_per_mile\"] >= 0.5) & (df[\"fare_per_mile\"] <= 25))\n",
    "    ]\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 6. Remove duplicate trips\n",
    "    # -------------------------------------------------------------------\n",
    "    # Define columns that uniquely identify a trip:\n",
    "    # pickup time, dropoff time, pickup location, dropoff location, total amount.\n",
    "    # If multiple identical rows exist, keep the first and drop the rest.\n",
    "    before = len(df)\n",
    "    dedup_cols = [\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\",\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"total_amount\"\n",
    "    ]\n",
    "    df = df.drop_duplicates(subset=dedup_cols, keep=\"first\")\n",
    "    removed = before - len(df)\n",
    "    print(f\"Deduplicated: removed {removed} rows\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # 7. Return the cleaned DataFrame\n",
    "    # -------------------------------------------------------------------\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ab2b6-4024-4ccb-aae8-c56ac4f926ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply the cleaning + feature engineering function to February data\n",
    "cleaned_feb = clean_add_features(df_feb)\n",
    "\n",
    "# 2. Define the warehouse output directory\n",
    "out_dir = \"data/warehouse/yellow_clean\"\n",
    "\n",
    "# 3. Save the cleaned February dataset to parquet files\n",
    "#    - engine=\"pyarrow\": needed for partitioned writes\n",
    "#    - index=False: don’t write the pandas index column\n",
    "#    - partition_cols=[\"year\",\"month\"]: split files into subfolders by year and month\n",
    "cleaned_feb.to_parquet(out_dir, engine=\"pyarrow\", index=False, partition_cols=[\"year\",\"month\"])\n",
    "\n",
    "# 4. Print a confirmation so you know where the data was written\n",
    "print(\"Cleaned Feb data saved under:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5dd94-b7f7-4dde-bb2e-7e32484d7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the raw February dataset using your cleaning + feature engineering function\n",
    "cleaned_feb = clean_add_features(df_feb)\n",
    "\n",
    "# Define the warehouse directory where partitioned parquet files will be stored\n",
    "out_dir = \"data/warehouse/yellow_clean\"\n",
    "\n",
    "# Save the cleaned February data into the warehouse\n",
    "# - engine=\"pyarrow\": parquet backend that supports partitioned writes\n",
    "# - index=False: do not include the DataFrame index in the parquet file\n",
    "# - partition_cols=[\"year\",\"month\"]: automatically organize into subfolders by year/month\n",
    "cleaned_feb.to_parquet(\n",
    "    out_dir,\n",
    "    engine=\"pyarrow\",\n",
    "    index=False,\n",
    "    partition_cols=[\"year\",\"month\"]\n",
    ")\n",
    "\n",
    "# Print a confirmation so you know where the cleaned files ended up\n",
    "print(\"Cleaned Feb data saved under:\", out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
