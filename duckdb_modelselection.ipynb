{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75aaed12-8209-4446-af22-abd95084d771",
   "metadata": {},
   "source": [
    "**Why Use DuckDB Instead of Only pandas**\n",
    "\n",
    "Speed on large datasets\n",
    "- DuckDB is optimized for analytics on columnar data like Parquet and runs faster than pandas when working with millions of rows.\n",
    "\n",
    "Query Parquet directly\n",
    "- You do not need to load entire files into memory. DuckDB queries Parquet partitions directly on disk.\n",
    "\n",
    "SQL interface\n",
    "- You can use familiar SQL (GROUP BY, EXTRACT, JOIN) inside Python, and return results as pandas DataFrames.\n",
    "\n",
    "Memory efficient\n",
    "- DuckDB only brings back the aggregated results instead of loading all rows into RAM.\n",
    "\n",
    "Scales easily\n",
    "-The same queries work for two months of data or multiple years of data without changing code.\n",
    "\n",
    "Complements pandas\n",
    "- Use pandas for smaller data exploration and custom calculations, and DuckDB for heavy queries and large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25f522-503e-4411-8468-84f7a5f6297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DuckDB package (only needed the first time in a new environment)\n",
    "!pip install duckdb\n",
    "\n",
    "# Import required libraries\n",
    "import duckdb       # DuckDB: in-process SQL engine for analytics\n",
    "import sys          # sys: lets us query system and Python runtime info\n",
    "import os           # os: for operating system utilities (file paths, env vars)\n",
    "import pathlib      # pathlib: modern, object-oriented filesystem paths\n",
    "\n",
    "# Print current Python version (just the first part, e.g., \"3.11.9\")\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "\n",
    "# Print installed DuckDB version (e.g., \"1.1.2\")\n",
    "print(\"DuckDB:\", duckdb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f030b-07bd-48d4-8dea-a524a96932bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DuckDB database file path (will create if it doesn’t exist yet)\n",
    "db_path = \"nyc.duckdb\"       \n",
    "\n",
    "# Connect to the DuckDB database file (creates file if missing)\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# Set a DuckDB runtime setting: allow queries to use up to 4 CPU threads\n",
    "con.execute(\"PRAGMA threads=4;\")  \n",
    "\n",
    "# Return two things:\n",
    "#   1. The database path string\n",
    "#   2. Whether the file currently exists on disk (True/False)\n",
    "db_path, os.path.exists(db_path)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "681b7ce5-0e0f-4c16-80c7-4db858682ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_ts</th>\n",
       "      <th>max_ts</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024-03-01 00:01:37</td>\n",
       "      <td>8719170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_ts              max_ts     rows\n",
       "0 2024-01-01 2024-03-01 00:01:37  8719170"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create (or replace) a DuckDB VIEW named \"yellow\"\n",
    "# This view points to the cleaned, partitioned Parquet files we saved earlier.\n",
    "# We use a glob path ('year=2024/month=*/**/*.parquet') to match all Jan+Feb parquet files.\n",
    "con.execute(\"\"\"\n",
    "  CREATE OR REPLACE VIEW yellow AS\n",
    "  SELECT *\n",
    "  FROM read_parquet('data/warehouse/yellow_clean/year=2024/month=*/**/*.parquet');\n",
    "\"\"\")\n",
    "\n",
    "# Run a quick sanity check SQL query:\n",
    "# - MIN(tpep_pickup_datetime): earliest pickup timestamp in the dataset\n",
    "# - MAX(tpep_pickup_datetime): latest pickup timestamp\n",
    "# - COUNT(*): total number of rows\n",
    "# Convert the DuckDB result to a pandas DataFrame with .df()\n",
    "con.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(tpep_pickup_datetime) AS min_ts, \n",
    "        MAX(tpep_pickup_datetime) AS max_ts, \n",
    "        COUNT(*) AS rows \n",
    "    FROM yellow\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d602a3bc-7a70-41c7-93ef-48051a4bf3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   hour   trips\n",
       " 0     0  233755\n",
       " 1     1  159178\n",
       " 2     2  108748\n",
       " 3     3   70245\n",
       " 4     4   47452,\n",
       "         ym    trips       revenue  avg_total\n",
       " 0  2024-01  2880425  7.859816e+07  27.287001\n",
       " 1  2024-02  5838741  1.586736e+08  27.175989,\n",
       "   payment_type_name    trips  total_revenue  avg_total\n",
       " 0       Credit card  6935089   1.940975e+08  27.987745\n",
       " 1              Cash  1203263   2.853025e+07  23.710738\n",
       " 2             Other   487106   1.235331e+07  25.360627\n",
       " 3           Dispute    64417   1.633888e+06  25.364236\n",
       " 4         No charge    29291   6.567632e+05  22.422014)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trips by hour --------------------------------------------\n",
    "trips_by_hour = con.sql(\"\"\"\n",
    "  SELECT \n",
    "    EXTRACT(hour FROM tpep_pickup_datetime) AS hour,  -- extract pickup hour (0–23)\n",
    "    COUNT(*) AS trips                                -- count total trips in that hour\n",
    "  FROM yellow\n",
    "  WHERE year=2024 AND month IN (1,2)                 -- filter Jan + Feb 2024 only\n",
    "  GROUP BY 1                                         -- group by the extracted hour\n",
    "  ORDER BY 1                                         -- sort ascending by hour\n",
    "\"\"\").df()                                            # convert DuckDB result to pandas DataFrame\n",
    "\n",
    "# Revenue by month -----------------------------------------\n",
    "rev_by_month = con.sql(\"\"\"\n",
    "  SELECT \n",
    "    strftime(tpep_pickup_datetime, '%Y-%m') AS ym,   -- format pickup datetime as \"YYYY-MM\"\n",
    "    COUNT(*) AS trips,                               -- number of trips that month\n",
    "    SUM(total_amount) AS revenue,                    -- total revenue\n",
    "    AVG(total_amount) AS avg_total                   -- average revenue per trip\n",
    "  FROM yellow\n",
    "  WHERE year=2024 AND month IN (1,2)                 -- restrict to Jan + Feb 2024\n",
    "  GROUP BY 1                                         -- group by formatted year-month\n",
    "  ORDER BY 1                                         -- sort chronologically by ym\n",
    "\"\"\").df()\n",
    "\n",
    "# Revenue by payment type -------------------------------\n",
    "rev_by_pay = con.sql(\"\"\"\n",
    "  SELECT\n",
    "    CASE payment_type                                -- map numeric codes to human labels\n",
    "      WHEN 1 THEN 'Credit card'\n",
    "      WHEN 2 THEN 'Cash'\n",
    "      WHEN 3 THEN 'No charge'\n",
    "      WHEN 4 THEN 'Dispute'\n",
    "      WHEN 5 THEN 'Unknown'\n",
    "      WHEN 6 THEN 'Voided trip'\n",
    "      ELSE 'Other' END AS payment_type_name,\n",
    "    COUNT(*) AS trips,                               -- number of trips by payment type\n",
    "    SUM(total_amount) AS total_revenue,              -- total revenue by payment type\n",
    "    AVG(total_amount) AS avg_total                   -- average trip total by payment type\n",
    "  FROM yellow\n",
    "  WHERE year=2024 AND month IN (1,2)                 -- limit to Jan + Feb 2024\n",
    "  GROUP BY 1                                         -- group by payment_type_name\n",
    "  ORDER BY total_revenue DESC                        -- rank by revenue, highest first\n",
    "\"\"\").df()\n",
    "\n",
    "# Show first few rows of each result to verify\n",
    "trips_by_hour.head(), rev_by_month.head(), rev_by_pay.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616be7b-e8a2-414f-85d9-bcd631c8ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import duckdb         # SQL-on-Parquet engine (in-process, fast)\n",
    "import pandas as pd   # DataFrames for model input/output\n",
    "import os             # to check file existence\n",
    "\n",
    "# Path to your existing DuckDB database file\n",
    "db_path = \"nyc.duckdb\"     # change if your file has a different name\n",
    "\n",
    "# Connect (creates the file if it doesn't exist)\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# Let DuckDB use multiple threads for speed (tweak as desired)\n",
    "con.execute(\"PRAGMA threads=4;\")\n",
    "\n",
    "# Confirm the DB file exists now\n",
    "print(\"DuckDB file:\", db_path, \"| exists:\", os.path.exists(db_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c38069-a87e-4120-9649-97d516e1bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing values each column has in the modeling DataFrame\n",
    "df.isna().sum()  # quick scan: look for non-zero counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea77d4-c47a-4c94-b43f-a3c3b904d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing target; keep only rows where tip_amount is present\n",
    "df_model = df.dropna(subset=[\"tip_amount\"]).copy()  # create a clean copy with target present\n",
    "\n",
    "# Define target and features\n",
    "y = df_model[\"tip_amount\"].astype(float)  # target: tip in USD\n",
    "X = df_model.drop(columns=[\"tip_amount\"]) # features: everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae42310-7158-4a4f-9082-177f45051fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # <-- imputer for NaNs\n",
    "\n",
    "# Identify numeric and categorical features \n",
    "numeric_features = [\"trip_distance\", \"duration_min\", \"passenger_count\", \"pickup_hour\", \"month\", \"dow\", \"is_weekend\"]\n",
    "categorical_features = [\"payment_type\", \"RatecodeID\"] + ([ \"PULocationID\", \"DOLocationID\" ] if \"PULocationID\" in X.columns else [])\n",
    "\n",
    "# Numeric pipeline: impute missing with median\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))  # fill NaNs in numeric columns with median\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute missing with most frequent, then one-hot encode\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),       # fill NaNs in categoricals with mode\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))             # one-hot encode; ignore unseen labels at predict time\n",
    "])\n",
    "\n",
    "# ColumnTransformer to apply pipelines to the right columns\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_features),            # apply numeric pipeline to numeric columns\n",
    "        (\"cat\", categorical_pipeline, categorical_features),    # apply categorical pipeline to categorical columns\n",
    "    ],\n",
    "    remainder=\"drop\"                                            # drop any columns not listed above\n",
    ")\n",
    "\n",
    "# Train/test split (random; for time-aware use Jan=train, Feb=test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8773e37-ea27-47b7-9361-db92525dae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and metrics\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define a few baseline regressors\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42),\n",
    "}\n",
    "\n",
    "# Fit/evaluate each model in a pipeline with preprocessing\n",
    "rows = []  # collect results here\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"model\", model)])  # preprocessing + estimator\n",
    "    pipe.fit(X_train, y_train)                                  # train\n",
    "    preds = pipe.predict(X_test)                                # predict on holdout\n",
    "\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)     # RMSE\n",
    "    mae  = mean_absolute_error(y_test, preds)                   # MAE\n",
    "    r2   = r2_score(y_test, preds)                              # R²\n",
    "\n",
    "    rows.append({\"model\": name, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
    "\n",
    "# Leaderboard\n",
    "results = pd.DataFrame(rows).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa8be3-b969-4a35-9437-d6ce4eb1f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define target and features\n",
    "y = df[\"tip_amount\"].astype(float)     # target: tip in USD\n",
    "X = df.drop(columns=[\"tip_amount\"])    # all other columns are features\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = [\"trip_distance\", \"duration_min\", \"passenger_count\", \"pickup_hour\", \"month\", \"dow\", \"is_weekend\"]\n",
    "categorical_features = [\"payment_type\", \"RatecodeID\"] + ([\"PULocationID\", \"DOLocationID\"] if use_locations else [])\n",
    "\n",
    "# Preprocess: pass numeric through, one-hot encode categoricals\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),                         # keep numeric as-is\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), categorical_features),  # OHE for cats\n",
    "    ],\n",
    "    remainder=\"drop\"                                                      # drop anything else\n",
    ")\n",
    "\n",
    "# Split data (random split; for time-aware, train=Jan, test=Feb)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f131378-91f8-4c64-8a2a-22564b1858b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and metrics\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define a few models to compare\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42),\n",
    "}\n",
    "\n",
    "# Train/evaluate each model inside a pipeline with preprocessing\n",
    "rows = []  # to collect results\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"model\", model)])  # preprocessing + estimator\n",
    "    pipe.fit(X_train, y_train)                                  # train\n",
    "    preds = pipe.predict(X_test)                                # predict on holdout\n",
    "\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)     # RMSE\n",
    "    mae  = mean_absolute_error(y_test, preds)                   # MAE\n",
    "    r2   = r2_score(y_test, preds)                              # R²\n",
    "\n",
    "    rows.append({\"model\": name, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
    "\n",
    "# Leaderboard sorted by RMSE (lower is better)\n",
    "results = pd.DataFrame(rows).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ff1cc-7513-46f9-8a77-6a948e16a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540bab3-6f63-4597-b5ff-3fef28b71717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e51dcf-3b27-4d9f-ad6c-3be0a7b8599a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
